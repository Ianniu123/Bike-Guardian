{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face detection and recognition training pipeline\n",
    "\n",
    "The following example illustrates how to fine-tune an InceptionResnetV1 model on your own dataset. This will mostly follow standard pytorch training patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1, fixed_image_standardization, training\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torchvision.utils import save_image\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = 0 if os.name == 'nt' else 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_pipeline = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    fixed_image_standardization,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = v2.Compose([\n",
    "    v2.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5.)),\n",
    "    v2.ColorJitter(brightness=.5, hue=.3),\n",
    "    v2.Grayscale(num_output_channels=3),\n",
    "    v2.RandomPosterize(bits=2),\n",
    "    v2.RandomPerspective(p=0.5),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    fixed_image_standardization,\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Define run parameters\n",
    "\n",
    "The dataset should follow the VGGFace2/ImageNet-style directory layout. Modify `data_dir` to the location of the dataset on wish to finetune on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images_to_generate = 50\n",
    "synthetic_images = []\n",
    "image_idx = 1\n",
    "for x, y in synthetic_dataset:\n",
    "    output_directory = \"./data/train/Ian/Ian0.jpg\"\n",
    "    save_image(x, output_directory)\n",
    "\n",
    "for i in range(num_images_to_generate):\n",
    "    for x, y in synthetic_dataset:\n",
    "        new_image = x\n",
    "        output_directory = \"./data/train/Ian/Ian\" + str(image_idx) + str(\".jpg\")\n",
    "        color_transformation_idx = np.random.randint(low=0, high=4)\n",
    "        color_transformation = None\n",
    "\n",
    "        if color_transformation_idx != 4:\n",
    "            color_transformation = color_transformations[color_transformation_idx]\n",
    "            new_image = color_transformation(new_image)\n",
    "\n",
    "        new_image = posture_transformations(new_image)\n",
    "\n",
    "        if not torch.equal(x, new_image):\n",
    "            save_image(new_image, output_directory)\n",
    "            image_idx +=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Define Inception Resnet V1 module\n",
    "\n",
    "See `help(InceptionResnetV1)` for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = \"./Training\"\n",
    "train_dataset = datasets.ImageFolder(train_data_dir, transform=read_pipeline)\n",
    "\n",
    "val_data_dir = \"./data/val\"\n",
    "val_dataset = datasets.ImageFolder(val_data_dir, transform=read_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Define optimizer, scheduler, dataset, and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(resnet.parameters(), lr=0.001)\n",
    "# scheduler = MultiStepLR(optimizer, [5, 10])\n",
    "\n",
    "img_inds = np.arange(len(train_dataset))\n",
    "np.random.shuffle(img_inds)\n",
    "train_inds = img_inds[:int(0.8 * len(train_dataset))]\n",
    "val_inds = img_inds[int(0.8 * len(train_dataset)):]\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    num_workers=workers,\n",
    "    batch_size=batch_size,\n",
    "    sampler=SubsetRandomSampler(train_inds)\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    num_workers=workers,\n",
    "    batch_size=batch_size,\n",
    "    sampler=SubsetRandomSampler(val_inds)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Define loss and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "metrics = {\n",
    "    'fps': training.BatchTimer(),\n",
    "    'acc': training.accuracy\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "writer.iteration, writer.interval = 0, 10\n",
    "\n",
    "print('\\n\\nInitial')\n",
    "print('-' * 10)\n",
    "resnet.eval()\n",
    "training.pass_epoch(\n",
    "    resnet, loss_fn, val_loader,\n",
    "    batch_metrics=metrics, show_running=True, device=device,\n",
    "    writer=writer\n",
    ")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('\\nEpoch {}/{}'.format(epoch + 1, epochs))\n",
    "    print('-' * 10)\n",
    "\n",
    "    resnet.train()\n",
    "    training.pass_epoch(\n",
    "        resnet, loss_fn, train_loader, optimizer,\n",
    "        batch_metrics=metrics, show_running=True, device=device,\n",
    "        writer=writer\n",
    "    )\n",
    "\n",
    "    resnet.eval()\n",
    "    training.pass_epoch(\n",
    "        resnet, loss_fn, val_loader,\n",
    "        batch_metrics=metrics, show_running=True, device=device,\n",
    "        writer=writer\n",
    "    )\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define MTCNN module\n",
    "### See help(MTCNN) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, identity_image, negative_dataset, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            anchor_image: A single image of the registered person (PIL or np.array).\n",
    "            negative_dataset: A dataset with multiple identities.\n",
    "            transform: Transformation for data augmentation.\n",
    "        \"\"\"\n",
    "        self.anchor_image = identity_image\n",
    "        self.negative_dataset = negative_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.negative_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Generate anchor and positive (augmented anchor)\n",
    "        if self.transform:\n",
    "            anchor = self.transform(self.anchor_image)\n",
    "            positive = self.transform(self.anchor_image)\n",
    "\n",
    "        # Get a random negative sample from the public dataset\n",
    "        negative = self.negative_dataset[idx][0]  # Assuming dataset returns (image, label)\n",
    "        if self.transform:\n",
    "            negative = self.transform(negative)\n",
    "\n",
    "        return anchor, positive, negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = InceptionResnetV1(\n",
    "    classify=False,\n",
    "    pretrained='vggface2',\n",
    ").to(device)\n",
    "resnet.logits= None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in resnet.last_linear.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in resnet.last_bn.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate dataset and dataloader\n",
    "anchor_image = Image.open(\"data\\original\\Ian.jpg\")\n",
    "negative_dataset = datasets.ImageFolder(\"./data/negative\", transform=read_pipeline)\n",
    "triplet_dataset = TripletDataset(anchor_image, negative_dataset, transform=transformations)\n",
    "dataloader = DataLoader(triplet_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "lr = 0.001\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0001\n",
    "    \n",
    "optimizer = torch.optim.SGD(resnet.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50], gamma=0.1)\n",
    "\n",
    "# Define loss function\n",
    "triplet_loss = nn.TripletMarginLoss(margin=0.5, p=2, eps=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "num_epochs = 50\n",
    "\n",
    "original_image = read_pipeline(Image.open(\"data\\original\\Ian.jpg\")).unsqueeze(0)\n",
    "test_image1 = read_pipeline(Image.open(\"data\\original\\Ian01.jpg\")).unsqueeze(0)\n",
    "test_image2 = read_pipeline(Image.open(\"./output.jpg\")).unsqueeze(0)\n",
    "\n",
    "resnet.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for anchor, positive, negative in dataloader:\n",
    "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        emb_anchor = resnet(anchor)\n",
    "        emb_positive = resnet(positive)\n",
    "        emb_negative = resnet(negative)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = triplet_loss(emb_anchor, emb_positive, emb_negative)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "    if (epoch == 50):\n",
    "        torch.save(resnet.state_dict(), \"./lr=0.001_batch_size=64_margin=0.5_epochs=50_glasses.pt\")\n",
    "\n",
    "    resnet.eval()\n",
    "    with torch.no_grad():\n",
    "        emb = resnet(original_image.to(device)).cpu().numpy().squeeze()\n",
    "        emb1 = resnet(test_image1.to(device)).cpu().numpy().squeeze()\n",
    "        emb2 = resnet(test_image2.to(device)).cpu().numpy().squeeze()\n",
    "        distance = euclidean(emb, emb1)\n",
    "        print(distance)\n",
    "        distance = euclidean(emb, emb2)\n",
    "        print(distance)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = datasets.ImageFolder(\"./data/negative\", transform=read_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "# Test pair of images\n",
    "test_image1 = read_pipeline(Image.open(\"data\\original\\Ian\\Ian02.jpg\")).unsqueeze(0)\n",
    "test_image2 = read_pipeline(Image.open(\"./data/val_cropped/Ian/test.jpg\")).unsqueeze(0)\n",
    "resnet.eval()\n",
    "count = 0\n",
    "for x, y in val_dataset:\n",
    "    test_image1 = read_pipeline(Image.open(\"data\\original\\Ian\\Ian02.jpg\")).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        emb1 = resnet(test_image1.to(device)).cpu().numpy().squeeze()\n",
    "        emb2 = resnet(x.unsqueeze(0).to(device)).cpu().numpy().squeeze()   \n",
    "\n",
    "    # Compute similarity\n",
    "    distance = euclidean(emb1, emb2)\n",
    "    if distance < 0.7:\n",
    "        print(f\"Distance: {distance}\")\n",
    "        count += 1\n",
    "\n",
    "print(f\"Accuracy: {1 - count / len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count/len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {1-count / len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Unused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "# Test pair of images\n",
    "test_image1 = read_pipeline(Image.open(\"data\\original\\Ian\\Ian02.jpg\")).unsqueeze(0)\n",
    "test_image2 = read_pipeline(Image.open(\"data\\original\\Ian\\Ian01.jpg\")).unsqueeze(0)\n",
    "\n",
    "# Generate embeddings\n",
    "resnet.eval()\n",
    "with torch.no_grad():\n",
    "    emb1 = resnet(test_image1.to(device)).cpu().numpy().squeeze()\n",
    "    emb2 = resnet(test_image2.to(device)).cpu().numpy().squeeze()\n",
    "\n",
    "# Compute similarity\n",
    "distance = euclidean(emb1, emb2)\n",
    "print(f\"Distance: {distance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(\n",
    "    image_size=160, margin=0, min_face_size=20,\n",
    "    thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(\"./data/val\")\n",
    "dataset.samples = [\n",
    "    (p, p.replace(\"./data/val\", \"./data/val_cropped\"))\n",
    "        for p, _ in dataset.samples\n",
    "]\n",
    "        \n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    num_workers=workers,\n",
    "    batch_size=1,\n",
    "    collate_fn=training.collate_pil\n",
    ")\n",
    "\n",
    "for i, (x, y) in enumerate(loader):\n",
    "    mtcnn(x, save_path=y)\n",
    "    print('\\rBatch {} of {}'.format(i + 1, len(loader)), end='')\n",
    "    \n",
    "# Remove mtcnn to reduce GPU memory usage\n",
    "del mtcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.load(\"./model.pt\", weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(resnet.state_dict(), \"./lr=0.001_batch_size=32_margin=0.5_epochs=50.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(resnet.state_dict(), \"./model2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
